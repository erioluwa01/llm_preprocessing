{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOTvWd3nzLkf+ZhTZ8EEtix",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/erioluwa01/llm_preprocessing/blob/main/positional_vector_embedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJvTuId9kDDn",
        "outputId": "9121ee4b-5de9-4401-8fea-ad03c9ed77ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.3.0.post1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n",
            "[ 1.25976562e-01  2.97851562e-02  8.60595703e-03  1.39648438e-01\n",
            " -2.56347656e-02 -3.61328125e-02  1.11816406e-01 -1.98242188e-01\n",
            "  5.12695312e-02  3.63281250e-01 -2.42187500e-01 -3.02734375e-01\n",
            " -1.77734375e-01 -2.49023438e-02 -1.67968750e-01 -1.69921875e-01\n",
            "  3.46679688e-02  5.21850586e-03  4.63867188e-02  1.28906250e-01\n",
            "  1.36718750e-01  1.12792969e-01  5.95703125e-02  1.36718750e-01\n",
            "  1.01074219e-01 -1.76757812e-01 -2.51953125e-01  5.98144531e-02\n",
            "  3.41796875e-01 -3.11279297e-02  1.04492188e-01  6.17675781e-02\n",
            "  1.24511719e-01  4.00390625e-01 -3.22265625e-01  8.39843750e-02\n",
            "  3.90625000e-02  5.85937500e-03  7.03125000e-02  1.72851562e-01\n",
            "  1.38671875e-01 -2.31445312e-01  2.83203125e-01  1.42578125e-01\n",
            "  3.41796875e-01 -2.39257812e-02 -1.09863281e-01  3.32031250e-02\n",
            " -5.46875000e-02  1.53198242e-02 -1.62109375e-01  1.58203125e-01\n",
            " -2.59765625e-01  2.01416016e-02 -1.63085938e-01  1.35803223e-03\n",
            " -1.44531250e-01 -5.68847656e-02  4.29687500e-02 -2.46582031e-02\n",
            "  1.85546875e-01  4.47265625e-01  9.58251953e-03  1.31835938e-01\n",
            "  9.86328125e-02 -1.85546875e-01 -1.00097656e-01 -1.33789062e-01\n",
            " -1.25000000e-01  2.83203125e-01  1.23046875e-01  5.32226562e-02\n",
            " -1.77734375e-01  8.59375000e-02 -2.18505859e-02  2.05078125e-02\n",
            " -1.39648438e-01  2.51464844e-02  1.38671875e-01 -1.05468750e-01\n",
            "  1.38671875e-01  8.88671875e-02 -7.51953125e-02 -2.13623047e-02\n",
            "  1.72851562e-01  4.63867188e-02 -2.65625000e-01  8.91113281e-03\n",
            "  1.49414062e-01  3.78417969e-02  2.38281250e-01 -1.24511719e-01\n",
            " -2.17773438e-01 -1.81640625e-01  2.97851562e-02  5.71289062e-02\n",
            " -2.89306641e-02  1.24511719e-02  9.66796875e-02 -2.31445312e-01\n",
            "  5.81054688e-02  6.68945312e-02  7.08007812e-02 -3.08593750e-01\n",
            " -2.14843750e-01  1.45507812e-01 -4.27734375e-01 -9.39941406e-03\n",
            "  1.54296875e-01 -7.66601562e-02  2.89062500e-01  2.77343750e-01\n",
            " -4.86373901e-04 -1.36718750e-01  3.24218750e-01 -2.46093750e-01\n",
            " -3.03649902e-03 -2.11914062e-01  1.25000000e-01  2.69531250e-01\n",
            "  2.04101562e-01  8.25195312e-02 -2.01171875e-01 -1.60156250e-01\n",
            " -3.78417969e-02 -1.20117188e-01  1.15234375e-01 -4.10156250e-02\n",
            " -3.95507812e-02 -8.98437500e-02  6.34765625e-03  2.03125000e-01\n",
            "  1.86523438e-01  2.73437500e-01  6.29882812e-02  1.41601562e-01\n",
            " -9.81445312e-02  1.38671875e-01  1.82617188e-01  1.73828125e-01\n",
            "  1.73828125e-01 -2.37304688e-01  1.78710938e-01  6.34765625e-02\n",
            "  2.36328125e-01 -2.08984375e-01  8.74023438e-02 -1.66015625e-01\n",
            " -7.91015625e-02  2.43164062e-01 -8.88671875e-02  1.26953125e-01\n",
            " -2.16796875e-01 -1.73828125e-01 -3.59375000e-01 -8.25195312e-02\n",
            " -6.49414062e-02  5.07812500e-02  1.35742188e-01 -7.47070312e-02\n",
            " -1.64062500e-01  1.15356445e-02  4.45312500e-01 -2.15820312e-01\n",
            " -1.11328125e-01 -1.92382812e-01  1.70898438e-01 -1.25000000e-01\n",
            "  2.65502930e-03  1.92382812e-01 -1.74804688e-01  1.39648438e-01\n",
            "  2.92968750e-01  1.13281250e-01  5.95703125e-02 -6.39648438e-02\n",
            "  9.96093750e-02 -2.72216797e-02  1.96533203e-02  4.27246094e-02\n",
            " -2.46093750e-01  6.39648438e-02 -2.25585938e-01 -1.68945312e-01\n",
            "  2.89916992e-03  8.20312500e-02  3.41796875e-01  4.32128906e-02\n",
            "  1.32812500e-01  1.42578125e-01  7.61718750e-02  5.98144531e-02\n",
            " -1.19140625e-01  2.74658203e-03 -6.29882812e-02 -2.72216797e-02\n",
            " -4.82177734e-03 -8.20312500e-02 -2.49023438e-02 -4.00390625e-01\n",
            " -1.06933594e-01  4.24804688e-02  7.76367188e-02 -1.16699219e-01\n",
            "  7.37304688e-02 -9.22851562e-02  1.07910156e-01  1.58203125e-01\n",
            "  4.24804688e-02  1.26953125e-01  3.61328125e-02  2.67578125e-01\n",
            " -1.01074219e-01 -3.02734375e-01 -5.76171875e-02  5.05371094e-02\n",
            "  5.26428223e-04 -2.07031250e-01 -1.38671875e-01 -8.97216797e-03\n",
            " -2.78320312e-02 -1.41601562e-01  2.07031250e-01 -1.58203125e-01\n",
            "  1.27929688e-01  1.49414062e-01 -2.24609375e-02 -8.44726562e-02\n",
            "  1.22558594e-01  2.15820312e-01 -2.13867188e-01 -3.12500000e-01\n",
            " -3.73046875e-01  4.08935547e-03  1.07421875e-01  1.06933594e-01\n",
            "  7.32421875e-02  8.97216797e-03 -3.88183594e-02 -1.29882812e-01\n",
            "  1.49414062e-01 -2.14843750e-01 -1.83868408e-03  9.91210938e-02\n",
            "  1.57226562e-01 -1.14257812e-01 -2.05078125e-01  9.91210938e-02\n",
            "  3.69140625e-01 -1.97265625e-01  3.54003906e-02  1.09375000e-01\n",
            "  1.31835938e-01  1.66992188e-01  2.35351562e-01  1.04980469e-01\n",
            " -4.96093750e-01 -1.64062500e-01 -1.56250000e-01 -5.22460938e-02\n",
            "  1.03027344e-01  2.43164062e-01 -1.88476562e-01  5.07812500e-02\n",
            " -9.37500000e-02 -6.68945312e-02  2.27050781e-02  7.61718750e-02\n",
            "  2.89062500e-01  3.10546875e-01 -5.37109375e-02  2.28515625e-01\n",
            "  2.51464844e-02  6.78710938e-02 -1.21093750e-01 -2.15820312e-01\n",
            " -2.73437500e-01 -3.07617188e-02 -3.37890625e-01  1.53320312e-01\n",
            "  2.33398438e-01 -2.08007812e-01  3.73046875e-01  8.20312500e-02\n",
            "  2.51953125e-01 -7.61718750e-02 -4.66308594e-02 -2.23388672e-02\n",
            "  2.99072266e-02 -5.93261719e-02 -4.66918945e-03 -2.44140625e-01\n",
            " -2.09960938e-01 -2.87109375e-01 -4.54101562e-02 -1.77734375e-01\n",
            " -2.79296875e-01 -8.59375000e-02  9.13085938e-02  2.51953125e-01]\n",
            "[('computers', 0.7979379892349243), ('laptop', 0.6640493273735046), ('laptop_computer', 0.6548868417739868), ('Computer', 0.647333562374115), ('com_puter', 0.6082080006599426), ('technician_Leonard_Luchko', 0.5662748217582703), ('mainframes_minicomputers', 0.5617720484733582), ('laptop_computers', 0.5585449934005737), ('PC', 0.5539618730545044), ('maker_Dell_DELL.O', 0.5519254207611084)]\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Install Gensim\n",
        "!pip install gensim\n",
        "\n",
        "# Step 2: Download the model using Gensim API\n",
        "import gensim.downloader as api\n",
        "model = api.load(\"word2vec-google-news-300\")  # ‚è≥ Will take time\n",
        "\n",
        "\n",
        "print(model['king'])\n",
        "print(model.most_similar('computer'))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_vectors = model\n",
        "#so lets see how the vector embeddings of a word looks like\n",
        "print(word_vectors['computers']) #This example is accessing the vector for the word 'computer'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pjHPlQwxlD8v",
        "outputId": "e4542499-b73b-41df-9682-bec47a63c9ff"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 0.32421875 -0.24316406  0.11523438  0.25976562 -0.18847656  0.10595703\n",
            " -0.10205078  0.10693359  0.28710938  0.01428223  0.0100708  -0.20214844\n",
            "  0.19238281  0.07714844 -0.03686523  0.06933594 -0.0013504   0.26757812\n",
            "  0.12011719  0.02746582 -0.0072937  -0.04443359  0.15625     0.10693359\n",
            "  0.1640625  -0.07177734  0.02355957 -0.03930664 -0.05004883 -0.17480469\n",
            " -0.06054688 -0.10839844 -0.17382812  0.01843262  0.14160156 -0.4140625\n",
            " -0.43554688 -0.12792969  0.1484375  -0.04882812 -0.11914062  0.23046875\n",
            "  0.265625    0.10400391  0.27929688  0.06933594 -0.03881836  0.31640625\n",
            " -0.40625     0.05712891 -0.01324463 -0.09960938  0.05737305 -0.18945312\n",
            " -0.15039062  0.23632812 -0.05102539 -0.17871094 -0.21972656  0.14746094\n",
            "  0.16308594  0.04736328 -0.13183594  0.22070312 -0.04003906  0.05517578\n",
            " -0.2734375   0.42773438 -0.25585938  0.06591797  0.05419922 -0.25\n",
            "  0.14453125 -0.00531006 -0.08984375 -0.01312256  0.08349609 -0.203125\n",
            " -0.0022583  -0.25390625  0.08935547  0.08447266  0.27539062  0.2890625\n",
            "  0.00595093 -0.15625     0.00994873  0.29882812 -0.04980469  0.11523438\n",
            "  0.11914062 -0.04052734 -0.05737305 -0.33203125  0.19238281 -0.18359375\n",
            "  0.11132812  0.20410156  0.21582031  0.10302734  0.2734375  -0.23535156\n",
            " -0.09912109  0.16699219  0.09619141  0.17871094 -0.14453125 -0.09472656\n",
            "  0.44140625  0.00062561 -0.11083984 -0.18945312 -0.09912109 -0.01361084\n",
            "  0.10449219  0.12451172 -0.00805664 -0.00817871  0.07861328  0.02722168\n",
            " -0.31445312  0.14160156 -0.11523438 -0.01281738 -0.13085938  0.06787109\n",
            " -0.18847656 -0.01525879  0.00552368  0.16601562  0.12890625 -0.3515625\n",
            "  0.02490234  0.16894531  0.09667969  0.13671875  0.07958984 -0.09228516\n",
            " -0.55859375 -0.25       -0.04248047 -0.27539062  0.14355469  0.02197266\n",
            "  0.05200195  0.01373291  0.2890625  -0.11083984 -0.21582031 -0.07958984\n",
            "  0.11816406  0.02807617 -0.14453125  0.19921875 -0.13085938  0.22265625\n",
            " -0.25       -0.00714111 -0.22753906  0.01940918 -0.06689453  0.05297852\n",
            " -0.11474609 -0.06933594  0.09521484  0.14160156 -0.11230469 -0.046875\n",
            " -0.22753906 -0.01574707 -0.08105469  0.09375    -0.15234375  0.11865234\n",
            " -0.04345703 -0.04760742 -0.05883789 -0.15136719 -0.234375   -0.10107422\n",
            " -0.04833984 -0.24121094 -0.07568359 -0.27539062  0.21582031  0.03710938\n",
            " -0.12304688  0.06445312  0.20996094 -0.07177734 -0.04003906 -0.01879883\n",
            " -0.16015625  0.20703125  0.03027344  0.25390625  0.15722656 -0.32617188\n",
            " -0.08300781 -0.05273438 -0.05102539  0.01324463  0.23925781  0.22558594\n",
            "  0.26171875 -0.03271484  0.10839844 -0.18652344 -0.33007812 -0.2421875\n",
            " -0.00081253  0.10400391 -0.16015625  0.04296875  0.17089844  0.25585938\n",
            " -0.13085938  0.37304688 -0.453125   -0.18945312 -0.09326172 -0.234375\n",
            "  0.07470703  0.22949219 -0.17578125 -0.07763672 -0.33789062  0.03955078\n",
            " -0.140625   -0.3046875  -0.09228516 -0.14648438 -0.07177734 -0.30273438\n",
            " -0.15625    -0.17578125  0.06542969 -0.14746094  0.09912109  0.14355469\n",
            "  0.20703125  0.05639648 -0.10058594 -0.03198242 -0.10693359 -0.03515625\n",
            " -0.11523438  0.09375     0.06689453 -0.01544189 -0.328125    0.25\n",
            " -0.23242188  0.078125   -0.11328125 -0.15136719  0.07519531  0.00762939\n",
            " -0.09033203  0.05249023 -0.02050781 -0.05151367  0.01831055  0.09033203\n",
            "  0.05761719  0.11669922 -0.13769531  0.19335938 -0.04370117  0.359375\n",
            " -0.08398438 -0.03417969  0.00369263  0.1484375   0.08105469 -0.15917969\n",
            "  0.18554688 -0.28515625  0.15917969  0.05615234 -0.07519531  0.10205078\n",
            " -0.01745605  0.125       0.10693359 -0.1484375   0.23535156  0.18261719\n",
            "  0.10546875  0.29492188 -0.35351562  0.01745605  0.03442383 -0.26171875\n",
            "  0.06176758 -0.23242188  0.01696777 -0.00604248 -0.02856445 -0.07275391]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(word_vectors['cat'].shape)"
      ],
      "metadata": {
        "id": "tYMmhca6p7uZ",
        "outputId": "d6fd4604-c9cc-4a42-a8ad-5a66caf01416",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(300,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(word_vectors.most_similar(positive=['king', 'woman'], negative=['man'], topn=10))"
      ],
      "metadata": {
        "id": "SpV1l2FYqDru",
        "outputId": "44f2505e-0f25-4c04-d81d-648e2b2a58b6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('queen', 0.7118193507194519), ('monarch', 0.6189674139022827), ('princess', 0.5902431011199951), ('crown_prince', 0.5499460697174072), ('prince', 0.5377321839332581), ('kings', 0.5236844420433044), ('Queen_Consort', 0.5235945582389832), ('queens', 0.5181134343147278), ('sultan', 0.5098593831062317), ('monarchy', 0.5087411999702454)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking the similarities between a pair of words too\n",
        "print(word_vectors.similarity('woman', 'man'))\n",
        "print(word_vectors.similarity('king', 'queen'))\n",
        "print(word_vectors.similarity('uncle', 'aunt'))\n",
        "print(word_vectors.similarity('boy', 'girl'))\n",
        "print(word_vectors.similarity('nephew', 'niece'))\n",
        "print(word_vectors.similarity('paper', 'water'))\n"
      ],
      "metadata": {
        "id": "1Ytd7MrsqICq",
        "outputId": "20b76763-90cf-4b6d-e52b-c0040100af6e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.76640123\n",
            "0.6510956\n",
            "0.7643474\n",
            "0.8543272\n",
            "0.7594368\n",
            "0.114080824\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#I can also find words similar to a given word\n",
        "print(word_vectors.most_similar(\"tower\", topn=5))\n"
      ],
      "metadata": {
        "id": "LzspIa0_qeQs",
        "outputId": "bdf71730-9a34-4295-e7dd-a391946721cf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('towers', 0.8531750440597534), ('skyscraper', 0.6417425870895386), ('Tower', 0.639177143573761), ('spire', 0.594687819480896), ('responded_Understood_Atlasjet', 0.5931612253189087)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#I can also check for similarities\n",
        "import numpy as np\n",
        "#words to compare\n",
        "\n",
        "word_1 = 'man'\n",
        "word_2 = 'woman'\n",
        "word_3 = 'semiconductor'\n",
        "word_4 = 'earthworm'\n",
        "word_5 = 'nephew'\n",
        "word_6 = 'niece'\n",
        "\n",
        "\n",
        "#Calculating the vector difference\n",
        "vector_difference1 = model[word_1] - model[word_2]\n",
        "vector_difference2 = model[word_3] - model[word_4]\n",
        "vector_difference3 = model[word_5] - model[word_6]\n",
        "\n",
        "#Calculating the magnitude of the vector difference\n",
        "magnitude_of_difference1 = np.linalg.norm(vector_difference1)\n",
        "magnitude_of_difference2 = np.linalg.norm(vector_difference2)\n",
        "magnitude_of_difference3 = np.linalg.norm(vector_difference3)\n",
        "\n",
        "#I want to print the magnitude of the difference\n",
        "print(\"The magnitude of the difference between '{}' and '{}' is {:.2f}\".format(word_1, word_2, magnitude_of_difference1))\n",
        "print(\"The magnitude of the difference between '{}' and '{}' is {:.2f}\".format(word_3, word_4, magnitude_of_difference2))\n",
        "print(\"The magnitude of the difference between '{}' and '{}' is {:.2f}\".format(word_5, word_6, magnitude_of_difference3))\n",
        "\n"
      ],
      "metadata": {
        "id": "EO3MoqaMqit1",
        "outputId": "7b943957-f5a6-446a-cc76-fee46e092438",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The magnitude of the difference between 'man' and 'woman' is 1.73\n",
            "The magnitude of the difference between 'semiconductor' and 'earthworm' is 5.67\n",
            "The magnitude of the difference between 'nephew' and 'niece' is 1.96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Creating token embeddings\n",
        "#SO i want to illustrate how converting token ids to vector embedding works by using the example of the token ids [2,3,5,1]"
      ],
      "metadata": {
        "id": "wf-x5mB7qtKi"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install torch\n",
        "import torch\n",
        "\n",
        "input_ids = torch.tensor([2,3,5,1])"
      ],
      "metadata": {
        "id": "gO7xAi1_qz1K"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 6\n",
        "output_dim = 3\n",
        "torch.manual_seed(123)\n",
        "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n"
      ],
      "metadata": {
        "id": "LzRu9ANqq2YW"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(embedding_layer.weight)"
      ],
      "metadata": {
        "id": "aTrRvLwgr0FJ",
        "outputId": "c7a10255-e54f-43e5-e7b8-6f0481b93c6a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[ 0.3374, -0.1778, -0.1690],\n",
            "        [ 0.9178,  1.5810,  1.3010],\n",
            "        [ 1.2753, -0.2010, -0.1606],\n",
            "        [-0.4015,  0.9666, -1.1481],\n",
            "        [-1.1589,  0.3255, -0.6315],\n",
            "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(embedding_layer(torch.tensor([3])))"
      ],
      "metadata": {
        "id": "wIangRCesEvJ",
        "outputId": "ba458829-e53d-480d-ee14-94137f5675ec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.4015,  0.9666, -1.1481]], grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(embedding_layer(input_ids))\n"
      ],
      "metadata": {
        "id": "4c7Faw-lsH04",
        "outputId": "920a5c8e-253e-41c2-d202-1dd284013d16",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 1.2753, -0.2010, -0.1606],\n",
            "        [-0.4015,  0.9666, -1.1481],\n",
            "        [-2.8400, -0.7849, -1.4096],\n",
            "        [ 0.9178,  1.5810,  1.3010]], grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ABz8GavCsmih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### A more realistic example .. exploring positional embeddings\n",
        "vocab_size=50252\n",
        "output_dim=256\n",
        "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n"
      ],
      "metadata": {
        "id": "M29JtySPsKtP"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_text = \"\"\"\n",
        "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no great surprise to me to hear that, in the height of his glory, he had dropped his painting, married a rich widow, and established himself in a villa on the Riviera. (Though I rather thought it would have been Rome or Florence.)\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "gbD7QP2Zsng9"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Step 1: Embedding layer parameters\n",
        "vocab_size = 50252\n",
        "output_dim = 256\n",
        "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
        "\n",
        "# Step 2: Simple hash-based tokenizer (simulating token IDs)\n",
        "def simple_tokenizer(text):\n",
        "    tokens = text.lower().split()\n",
        "    token_ids = [hash(word) % vocab_size for word in tokens]\n",
        "    return token_ids\n",
        "\n",
        "# Step 3: Dataset class for text chunks (input/target pairs)\n",
        "class TextDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, text, max_length, stride):\n",
        "        self.token_ids = simple_tokenizer(text)\n",
        "        self.samples = []\n",
        "        for i in range(0, len(self.token_ids) - max_length, stride):\n",
        "            input_seq = self.token_ids[i:i+max_length]\n",
        "            target_seq = self.token_ids[i+1:i+max_length+1]\n",
        "            self.samples.append((torch.tensor(input_seq), torch.tensor(target_seq)))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.samples[idx]  # (inputs, targets)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "# Step 4: Function to create dataloader\n",
        "def create_dataloader_v1(text, batch_size, max_length, stride, shuffle):\n",
        "    dataset = TextDataset(text, max_length, stride)\n",
        "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
        "\n",
        "# Step 5: Define your raw input text\n",
        "raw_text = \"\"\"\n",
        "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no great surprise to me to hear that, in the height of his glory, he had dropped his painting, married a rich widow, and established himself in a villa on the Riviera. (Though I rather thought it would have been Rome or Florence.)\n",
        "\"\"\"\n",
        "\n",
        "# Step 6: Create the dataloader\n",
        "max_length = 4\n",
        "dataloader = create_dataloader_v1(\n",
        "    raw_text, batch_size=8, max_length=max_length,\n",
        "    stride=max_length, shuffle=False\n",
        ")\n",
        "\n",
        "# Step 7: Fetch a batch from the dataloader\n",
        "data_iter = iter(dataloader)\n",
        "inputs, targets = next(data_iter)\n",
        "\n",
        "# Step 8: Print shapes and content\n",
        "print(\" Input IDs:\\n\", inputs)\n",
        "print(\"Shape:\", inputs.shape)  # Expected: [8, 4]\n",
        "\n",
        "print(\"\\n Target IDs:\\n\", targets)\n",
        "print(\"Shape:\", targets.shape)  # Expected: [8, 4]\n",
        "\n",
        "# Step 9: Apply embedding\n",
        "embedded_inputs = token_embedding_layer(inputs)\n",
        "print(\"\\n Embedded shape:\", embedded_inputs.shape)  # Expected: [8, 4, 256]\n"
      ],
      "metadata": {
        "id": "AfOdLiclsOeo",
        "outputId": "afb0f1cf-c72a-40cc-fd02-15428b7396e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Input IDs:\n",
            " tensor([[ 1027, 49411, 48580, 29002],\n",
            "        [13290, 43054, 37923, 34031],\n",
            "        [17575, 38639, 34031, 24096],\n",
            "        [ 6164, 17643, 21333, 48938],\n",
            "        [ 9076,  6141,  8825, 11290],\n",
            "        [39917, 11290, 38430, 34087],\n",
            "        [35518, 19793, 44362, 41725],\n",
            "        [41805, 36983, 46409, 49411]])\n",
            "Shape: torch.Size([8, 4])\n",
            "\n",
            " Target IDs:\n",
            " tensor([[49411, 48580, 29002, 13290],\n",
            "        [43054, 37923, 34031, 17575],\n",
            "        [38639, 34031, 24096,  6164],\n",
            "        [17643, 21333, 48938,  9076],\n",
            "        [ 6141,  8825, 11290, 39917],\n",
            "        [11290, 38430, 34087, 35518],\n",
            "        [19793, 44362, 41725, 41805],\n",
            "        [36983, 46409, 49411, 15826]])\n",
            "Shape: torch.Size([8, 4])\n",
            "\n",
            " Embedded shape: torch.Size([8, 4, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## now the token ID tensor is 8x4-dimensional, meaning the data batch consist of 8 text samples with 4 tokens\n",
        "## Now I want to use the embedding layer toe embed these token IDs into 256-dimensional vectors\n",
        "token_embeddings = token_embedding_layer(inputs)\n",
        "print(token_embeddings.shape)"
      ],
      "metadata": {
        "id": "_TlEoT0MsR89",
        "outputId": "c8a5fcae-915f-4b3d-e0cc-88f0de87639f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 4, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##Now I have to add a positional embedding vector to each of those vectors\n",
        "context_length = max_length\n",
        "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)"
      ],
      "metadata": {
        "id": "CfnZtbhRuH_b"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos_embeddings = pos_embedding_layer(torch.arange(max_length))\n",
        "print(pos_embeddings.shape)"
      ],
      "metadata": {
        "id": "wyWKF25NuRmo",
        "outputId": "1bc20159-568f-41f7-94a5-c09bcd797815",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Note that i want to add one postion vector to each of the 4 tokens ..\n",
        "#The same positional embeddings are applied to each input of 4 tokens (because there are only 4 positions)\n",
        "\n",
        "## As shown in the preceding code example, the input to the pos_embeddings is usually a\n",
        "## Pladehold of sequence of numbers 0,1,.., up to the maximum input length - 1\n",
        "## The context_length is a variable that represents the supported input size of the llm\n",
        "## Here, I chose it simila to the maximum length of the input text\n",
        "## In practice, the input text can be longer than the supported context length in which case we have to truncate\n",
        "\n",
        "# The positonal embedding tensor consis of four 256-dimensional vectors. We can now add these directly to the token embedding\n"
      ],
      "metadata": {
        "id": "ziLAOLEivBO8"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pa1H58gE0l8P"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}